{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c64267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import ast\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Maybe not needed?\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "import sklearn\n",
    "\n",
    "sklearn.__version__\n",
    "\n",
    "# TODO:\n",
    "    # [ ] Preprocessing\n",
    "    # [ ] Unieke woorden tellen\n",
    "    # [ ] TF*IDF\n",
    "    # [ ] Clusteren\n",
    "    # [ ] Bekijk topic per cluster\n",
    "    # [ ] Visualiseer evolutie  over tijd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file and create table\n",
    "records = []\n",
    "with open(\"data/data_mining_publications.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            records.append(ast.literal_eval(line))\n",
    "\n",
    "\n",
    "table = pd.DataFrame(records)\n",
    "\n",
    "# Get all titles and dates\n",
    "titles = table['title'].to_list()\n",
    "timestamps = table['year'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer\n",
    "ps: PorterStemmer = PorterStemmer()\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Add common words domain specific words to list of stop words\n",
    "domain_stops = {'approach', 'data', 'learn', 'use'}\n",
    "# Get English stop words and punctuation dict\n",
    "stop_words = set(stopwords.words('english')) | domain_stops\n",
    "punct = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "filtered_titles: list[str] = []\n",
    "\n",
    "for title in titles:\n",
    "    # Set string to lowercase and remove punctuation\n",
    "    title_words = word_tokenize(title.lower().translate(punct))\n",
    "\n",
    "    # Remove stopwords and stem each word\n",
    "    # filtered_title = [ps.stem(word) for word in title_words if word not in stop_words]\n",
    "    filtered_title = [wnl.lemmatize(word, pos=\"v\") for word in title_words if wnl.lemmatize(word, pos=\"v\") not in stop_words]\n",
    "#     filtered_title = [wnl.lemmatize(word, pos=\"v\") for word in title_words]\n",
    "    filtered_titles.append(\" \".join(filtered_title))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part creates a TF*IDF matrix\n",
    "\n",
    "vec = TfidfVectorizer(lowercase=False,  # already lowercased/stemmed tokens\n",
    "                      token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "                      use_idf=True,\n",
    "                      norm='l2',\n",
    "                      stop_words='english',\n",
    "                      min_df=2, # discard words which only appear once -> reduces vocab by ~7k words!\n",
    "                      # max_df=0.04\n",
    "                      )\n",
    "\n",
    "tfidf_matrix = vec.fit_transform(filtered_titles)\n",
    "vocab = vec.get_feature_names_out()\n",
    "print(\"n docs, vocab size:\", tfidf_matrix.shape)\n",
    "print(f\"{tfidf_matrix.getnnz() / np.prod(tfidf_matrix.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TF*IDF matrix\n",
    "i = 0\n",
    "row = tfidf_matrix.getrow(i)\n",
    "indices = row.indices\n",
    "data = row.data\n",
    "top_n = 10\n",
    "order = data.argsort()[::-1][:top_n]\n",
    "for pos in order:\n",
    "    print(vocab[indices[pos]], data[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e663885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 10\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k, \n",
    "    # random_state=0,\n",
    "    n_init=5)\n",
    "labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# top terms per cluster (cluster_centers_ is dense)\n",
    "order = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "for ci in range(k):\n",
    "    print(\"cluster\", ci, [vocab[i] for i in order[ci, :10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43461d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def elbow_plot():\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(100, 2000, 100)\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(k_range, inertias, 'bo-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "def silhouette_test():\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, 15)\n",
    "\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "        labels = kmeans.fit_predict(tfidf_matrix)\n",
    "        score = silhouette_score(tfidf_matrix, labels)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"k={k}: silhouette={score:.3f}\")\n",
    "\n",
    "    best_k = k_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "    print(f\"Best k: {best_k}\")\n",
    "\n",
    "    plt.plot(k_range, silhouette_scores, 'go-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.show()\n",
    "\n",
    "silhouette_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
